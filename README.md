# P-UCT
A novel parallel UCT algorithm with linear speedup and negligible performance loss. This package provide demo on Atari games (see [Run on your own environments](#Run-on-your-own-environments)） To allow easy extension to other environments, we include an environment [wrapper file](./Env/EnvWrapper.py)

# Introduction
Note: For full details of P-UCT, please refer to our [Arxiv](https://arxiv.org) paper.

## Conceptual idea
<p align="center">
<img src="Figures/Figure_puct_conceptual_idea.png" width="700">
</p>

We use the above figure to demonstrate the main problem caused by parallelizing UCT. (a) illustrates the four main steps of UCT: selection, expansion, simulation, and backpropagation. (b) a demonstration of the ideal (but unrealistic) parallel algorithm, i.e., the return V (cumulative reward) is available as soon as simulations start (in real-world cases they are observable only after simulations complete). (c) if we parallelize UCT naively, problems such as *collapes of exploration* or *exploitation failure* will happen. Specifically, since less statistics are available at the selection step, the algorithm cannot choose the "best" node to query. (d) we propose to keep track of the on-going but non-terminated simulations (called unobserved samples) to correct and compensate the outdated statistics. This allows performing principled selection step on parallel settings, allowing P-UCT to achieve linear speedup as well as negligible performance loss.

<p align="center">
<img src="Figures/Figure_tap_results.png" width="700">
</p>

P-UCT achieves ideal speedup under up to 16 workers, also without performance degradation.

<p align="center">
<img src="Figures/Figure_atari_results.png" width="700">
</p>

Clear advantage compared to baseline parallel approaches, in terms of both speed and accuracy.

## Implementation
<p align="center">
<img src="Figures/Figure_puct_pipeline.png" width="700">
</p>

Our implementation of the system consists of a master process and two sets of slave workers, i.e., expansion workers and simulation workers. With a clear division of labor, we parallel the most time-consuming expansion and simulation step, while maintain the sequential structure in the selection and backpropagation step.

<p align="center">
<img src="Figures/Figure_time_consumption.png" width="700">
</p>

The breakdown of time consumption (tested with 16 expansion and simulation workers) indicates we successfully parallel the most time-consuming expansion and simulation process and maintains time-consumption of other steps relatively small.

# Usage
## Prerequisites
- Python 3.x
- PyTorch 1.0
- Gym (with atari) 0.14.0
- Numpy 1.17.2
- Scipy 1.3.1
- OpenCV-Python 4.1.1.26

## Running
1. Download or clone the repository.
2. Run with the default settings:
```
  python3 main.py --model P-UCT
```
3. For additional hyperparameters please have a look at [main.py](./main.py) (they are also listed below), where descriptions are also included. For example, if you want to run the game PongNoFrameskip-v0 with 200 MCTS rollouts, simply run:
```
  python3 main.py --model P-UCT --env-name PongNoFrameskip-v0 --MCTS-max-steps 200
```
or if you want to record the video of gameplay, run:
```
  python3 main.py --model P-UCT --env-name PongNoFrameskip-v0 --record-video
```

* A full list of parameters
  * --model: MCTS model to use (currently support P-UCT and UCT).
  * --env-name: name of the environment.
  * --MCTS-max-steps: number of simulation steps in the planning phase.
  * --MCTS-max-depth: maximum planning depth.
  * --MCTS-max-width: maximum width for each node.
  * --gamma: environment discount factor.
  * --expansion-worker-num: number of expansion workers.
  * --simulation-worker-num: number of simulation workers.
  * --seed: random seed for the environment.
  * --max-episode-length: a strict upper bound of environment's episode length.
  * --policy: default policy (see above).
  * --device: support "cpu", "cuda:x", and "cuda". If entered "cuda", it will use all available cuda devices. Usually used to load the policy.
  * --record-video: see above.
  * --mode: MCTS or Distill, see [Planning with prior policy](#Planning-with-prior-policy).

### Planning with prior policy
The code currently support three default policies (policy used to perform simulation): *Random*, *PPO*, *DistillPPO* (to use them, change the “--policy” parameter). To use the *PPO* and *DistillPPO* policy, corresponding policy files need to be put in [./Policy/PPO/PolicyFiles](./Policy/PPO/PolicyFiles). PPO policy files can be generated by [Atari_PPO_training](./Utils/Atari_PPO_training). For example, by running
```
  cd Utils/Atari_PPO_training
  python3 main.py PongNoFrameskip-v0
```
a policy file will be generated in [./Utils/Atari_PPO_training/save](./Utils/Atari_PPO_training/save). To run DistillPPO, we have to run the distill training process by
```
  python3 main.py --mode Distill --env-name PongNoFrameskip-v0
```

## Run on your own environments
We kindly provide an [environment wrapper](./Env/EnvWrapper.py) to make easy extensions to other environments. All you need is to modify [./Env/EnvWrapper.py](./Env/EnvWrapper.py) and fit in your own environment. Specifically, you just need to rewrite APIs in the EnvWrapper class.

# Updates and to-do list
## Past updates
(currently empty)

## To-do list
1. Refactor prior policy module to support easy reuse.

# Reference
Please cite the paper in the following format if you used this code during your research :)
```
@inproceedings{
  anonymous2020watch,
  title={Watch the Unobserved: A Simple Approach to Parallelizing Monte Carlo Tree Search},
  author={Anonymous},
  booktitle={Submitted to International Conference on Learning Representations},
  year={2020},
  url={https://openreview.net/forum?id=BJlQtJSKDB},
  note={under review}
}
```
